<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Imaginative-Walks</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="icon" type="image/png" href="img/seal_icon.png">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
<div class="container" id="main">
    <div class="row">

        <h2 class="col-md-12 text-center" style="padding-bottom:20px">

            <b>Imaginative Walks: Generative Random Walk Deviation Loss for Improved Unseen Learning Representation</br></b>
            <span style="font-size:18pt"> arXiv preprint </span>
            <br>
        </h2>

    </div>
    <div class="row">
        <div class="col-md-12 text-center">
            <ul class="list-inline" style="font-size:18pt">
                    Anonymous
                <br>
            </ul>
        </div>
    </div>


    <div class="row" style="padding-top:45px">
        <div class="col-md-6 col-md-offset-3 text-center">
            <ul class="nav nav-pills nav-justified">
                <li>
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf">
                        <h4><strong>[ Paper ]</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="#video">
                        <h4><strong>[ Video ]</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="https://github.com/Vision-CAIR/GRaWD">
                        <h4><strong>[ Code ]</strong></h4>
                    </a>
                </li>
            </ul>
        </div>
    </div>


    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Abstract</b>
            </h3>
            <p class="text-justify">
                We propose a novel loss for generative models, dubbed as GRaWD (Generative Random Walk  Deviation), to improve learning representations of unexplored visual spaces.  Quality learning representation of unseen classes (or styles) is crucial to facilitate novel image generation and better generative understanding of unseen visual classes (a.k.a. Zero-Shot Learning, ZSL). By generating representations of unseen classes from their semantic descriptions, such as attributes or text, Generative ZSL aims at identifying unseen categories discriminatively from seen ones.    
                
                We define GRaWD by constructing a dynamic graph, including the seen class/style centers and generated samples in the current mini-batch.  Our loss starts a random walk probability from each center through visual generations produced from hallucinated unseen classes. As a deviation signal, we encourage the random walk to eventually land after t steps in a feature representation that is hard to classify to any of the seen classes. We show that our loss can improve unseen class representation quality on four text-based  ZSL benchmarks on CUB and NABirds datasets and three attribute-based ZSL benchmarks on AWA2, SUN, and aPY datasets. We also study our loss's ability to produce meaningful novel visual art generations on WikiArt dataset. Our experiments and human studies show that our loss can improve StyleGAN1 and StyleGAN2  generation quality, creating novel art that is significantly more preferred. Code will be made available.
            </p>
        </div>
    </div>


    <div class="row" id="video" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Video</b>
            </h3>
<!--            <video id="v0" width="100%" loop="" muted="" controls="">-->
<!--                <source src="img/hi_res.mp4" type="video/mp4">-->
<!--            </video>-->
            <iframe width="100%" height="400"
                src="https://www.youtube.com/embed/yEdf24hF_sY">
            </iframe>

        </div>

    </div>

    <div class="row" id="dataset" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Dataset</b>
            </h3>
            <ul>
                <li> You can download Nr3D <a href="https://drive.google.com/file/d/1qswKclq4BlnHSGMSgzLmUu8iqdUXD8ZC/view">here</a> (10.7MB) and Sr3D/Sr3D+ <a href="https://drive.google.com/drive/folders/1DS4uQq7fCmbJHeE-rEbO8G1-XatGEqNV">here</a> (19MB / 20MB). </li>
            </ul>
            <h4><b>Browse</b></h4>
            <ul>
                <li> You can browse Nr3D on ScanNet using our <a href="./data_browser.html"> browser</a>.</li>
            </ul>

            <br>
        </div>
    </div>

    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Method: ReferIt3DNet</b>
            </h3>
        </div>

        <div class="col-md-8 col-md-offset-2">
            <figure>
                <img src="img/method.png" style="padding-bottom:10px" class="img-responsive" alt="overview">
                    <figcaption>
                    </figcaption>
            </figure>
        </div>

        <div class="col-md-8 col-md-offset-2">
            Each object of a 3D scene, represented as a 6D point cloud containing its xyz coordinates and RGB color,
            is encoded by a visual encoder (e.g., PointNet++), with shared weights. Simultaneously, the utterance
            describing the referred object (e.g., “<i>the armchair next to the whiteboard</i>”) is processed by a Recurrent Neural
            Network (RNN). The resulting representations are fused together and processed by a Dynamic Graph Convolution
            Network (DGCN) which creates an object-centric <i>and</i> scene- (context-) aware representation per object.
            The output of the DGCN is processed by an MLP classifier that estimates the likelihood of each object to be
            the referred one. Two auxiliary losses modulate the unfused representations before these are processed by the DGCN
            via an object-class classifier and a text classifier respectively.
        </div>
    </div>


    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Qualitative Results</b>
            </h3>
        </div>

        <div class="col-md-8 col-md-offset-2">
            <figure>
                <img src="img/listener_qualitative_res.png" style="padding-bottom:10px" class="img-responsive" alt="overview">
                <figcaption>
                </figcaption>
            </figure>
        </div>

        <div class="col-md-8 col-md-offset-2">
            Successful cases of applying <b><i>ReferIt3DNet</i></b> are shown in the top four images and failure
            ones in the bottom two. Targets are shown in <span style="color:green">green</span> boxes, intra-class
            distractors in <span style="color:red">red</span>, and the referential text is displayed under each image.
            The network predictions are shown inside dashed <span style="color:yellow">yellow</span> circles, along with the
                inferred probabilities. We omit the probabilities of inter-class distractors to ease the presentation.
        </div>
    </div>

    <div class="row" id="citation" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Citation</b>
            </h3>
            If you find our work useful in your research, please consider citing:
<pre class="w3-panel w3-leftbar w3-light-grey">
@article{achlioptas2020referit_3d,
    title={ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes},
    author={Achlioptas, Panos and Abdelreheem, Ahmed and Xia, Fei and Elhoseiny, Mohamed and Guibas, Leonidas},
    journal={16th European Conference on Computer Vision (ECCV)},
    year={2020}
}</pre>
        </div>
    </div>

    <div class="row" id="benchmark" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>ReferIt3D Benchmark Challenge</b>
            </h3>
            Coming soon!
        </div>

    </div>


    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Acknowledgements</b>
            </h3>
            <p class="text-justify">
                The authors wish to acknowledge the support of a Vannevar Bush Faculty Fellowship, a grant from the
                Samsung GRO program and the Stanford SAIL Toyota Research Center, NSF grant IIS-1763268, KAUST grant
                BAS/1/1685-01-01, and a research gift from Amazon Web Services. The website template was borrowed from
                <a href="http://mgharbi.com/">Michaël Gharbi</a>.
            </p>
        </div>
    </div>


    </div>
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=110&t=tt&d=_kJ7hJdlh3UTdIJueDububmhQbOOTRZpo-A1RUHuEqU&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
    </body>

</html>
